---
title: 'From K-NN, HNSW to Product Quantization: Approximate Nearest Neighbor Search for vector search engines'
description: ''
tags: ['Approximate Nearest Neighbor Search', 'Product Quantization', 'Computer Vision']
pubDate: 'Oct 12 2025'
heroImage: '../../assets/blog-placeholder-5.jpg'
---

## Why do you need ANN? | Background

Elastic Search, Faiss and etc. all adopted their own approximate nearest neighbor search algorithms.
During my near research for my final-year project (FYP), which is to develop a production-level distributed search engine,
I have looked into **vector search**, **inverted index** and **Learning-to-Rank**, for my main and the most used features -- searching . 

This article is going to mostly cover vector search parts so we do not discuss about inverted index and Learning-to-Rank.

## Starting from K-NN
It is not a secret that K-Nearest Neighbors (K-NN) is the simplest and most naive nearest neighbor search algorithm.
However, when the vector dimension explodes or the number of vectors vastly increases,
K-NN algorithm reveals weakness in terms of its time complexity O(n^2) to inevitably calculate the euclidean distance for each and every vector in the dataset.

> For the references

Then following searching, Elastic search uses HNSW algorithm


